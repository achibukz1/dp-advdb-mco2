"""
Database Configuration and Connection Module - Multi-Node Support

This module handles all database connections for the Financial Reports Dashboard.
It supports both local MySQL connections and Google Cloud SQL connections for 3 nodes.
Works with both Streamlit secrets and .env files.
Uses st.connection() for better connection management when running in Streamlit.
Includes distributed locking functionality for coordinating transactions across nodes.
"""

import mysql.connector
import pandas as pd
import hashlib
from datetime import datetime
from dotenv import load_dotenv
import os
from typing import Dict, Any, List

# Load environment variables from .env file
load_dotenv()

# Cloud SQL connector will be initialized only when needed
_connector = None
_streamlit_connections = {}  # Cache for st.connection per node

def _is_running_in_streamlit():
    """
    Check if code is running inside a Streamlit app.

    Returns:
        bool: True if running in Streamlit, False otherwise
    """
    try:
        import streamlit as st
        return hasattr(st, 'secrets')
    except ImportError:
        return False


def _get_config_value(key, default=''):
    """
    Get configuration value from Streamlit secrets or environment variables.
    Streamlit secrets take precedence if available.

    Args:
        key (str): Configuration key name
        default: Default value if key not found

    Returns:
        Configuration value
    """
    try:
        # Try to import streamlit and use secrets
        import streamlit as st
        if hasattr(st, 'secrets') and key in st.secrets:
            return st.secrets[key]
    except (ImportError, FileNotFoundError, KeyError):
        pass

    # Fall back to environment variables
    return os.getenv(key, default)


# Query Cache Configuration
CACHE_ENABLED = str(_get_config_value('CACHE_ENABLED', 'True')).lower() == 'true'
CACHE_TTL_SECONDS = int(_get_config_value('CACHE_TTL_SECONDS', '3600'))
_query_cache = {}  # In-memory cache storage per node

# Database Configuration
# Choose connection method by setting USE_CLOUD_SQL environment variable
USE_CLOUD_SQL = str(_get_config_value('USE_CLOUD_SQL', 'False')).lower() == 'true'

# Node 1 Configuration
CLOUD_SQL_CONFIG_NODE1 = {
    "host": _get_config_value('CLOUD_DB_HOST'),
    "port": int(_get_config_value('CLOUD_DB_PORT', '3306')),
    "user": _get_config_value('CLOUD_DB_USER'),
    "password": _get_config_value('CLOUD_DB_PASSWORD'),
    "database": _get_config_value('CLOUD_DB_NAME')
}

LOCAL_CONFIG_NODE1 = {
    "host": _get_config_value('LOCAL_DB_HOST'),
    "port": int(_get_config_value('LOCAL_DB_PORT', '3306')),
    "user": _get_config_value('LOCAL_DB_USER'),
    "password": _get_config_value('LOCAL_DB_PASSWORD'),
    "database": _get_config_value('LOCAL_DB_NAME')
}

# Node 2 Configuration
CLOUD_SQL_CONFIG_NODE2 = {
    "host": _get_config_value('CLOUD_DB_HOST_NODE2', _get_config_value('CLOUD_DB_HOST')),
    "port": int(_get_config_value('CLOUD_DB_PORT_NODE2', '3306')),
    "user": _get_config_value('CLOUD_DB_USER_NODE2', _get_config_value('CLOUD_DB_USER')),
    "password": _get_config_value('CLOUD_DB_PASSWORD_NODE2', _get_config_value('CLOUD_DB_PASSWORD')),
    "database": _get_config_value('CLOUD_DB_NAME_NODE2', 'node2_db')
}

LOCAL_CONFIG_NODE2 = {
    "host": _get_config_value('LOCAL_DB_HOST_NODE2', 'localhost'),
    "port": int(_get_config_value('LOCAL_DB_PORT_NODE2', '3307')),
    "user": _get_config_value('LOCAL_DB_USER_NODE2', _get_config_value('LOCAL_DB_USER')),
    "password": _get_config_value('LOCAL_DB_PASSWORD_NODE2', _get_config_value('LOCAL_DB_PASSWORD')),
    "database": _get_config_value('LOCAL_DB_NAME_NODE2', 'node2_db')
}

# Node 3 Configuration
CLOUD_SQL_CONFIG_NODE3 = {
    "host": _get_config_value('CLOUD_DB_HOST_NODE3', _get_config_value('CLOUD_DB_HOST')),
    "port": int(_get_config_value('CLOUD_DB_PORT_NODE3', '3306')),
    "user": _get_config_value('CLOUD_DB_USER_NODE3', _get_config_value('CLOUD_DB_USER')),
    "password": _get_config_value('CLOUD_DB_PASSWORD_NODE3', _get_config_value('CLOUD_DB_PASSWORD')),
    "database": _get_config_value('CLOUD_DB_NAME_NODE3', 'node3_db')
}

LOCAL_CONFIG_NODE3 = {
    "host": _get_config_value('LOCAL_DB_HOST_NODE3', 'localhost'),
    "port": int(_get_config_value('LOCAL_DB_PORT_NODE3', '3308')),
    "user": _get_config_value('LOCAL_DB_USER_NODE3', _get_config_value('LOCAL_DB_USER')),
    "password": _get_config_value('LOCAL_DB_PASSWORD_NODE3', _get_config_value('LOCAL_DB_PASSWORD')),
    "database": _get_config_value('LOCAL_DB_NAME_NODE3', 'node3_db')
}

# Map node numbers to configurations
NODE_CONFIGS = {
    1: {"cloud": CLOUD_SQL_CONFIG_NODE1, "local": LOCAL_CONFIG_NODE1},
    2: {"cloud": CLOUD_SQL_CONFIG_NODE2, "local": LOCAL_CONFIG_NODE2},
    3: {"cloud": CLOUD_SQL_CONFIG_NODE3, "local": LOCAL_CONFIG_NODE3}
}

# Map node numbers to Streamlit connection names
STREAMLIT_CONN_NAMES = {
    1: {"cloud": "mysql", "local": "mysql_local"},
    2: {"cloud": "mysql_node2", "local": "mysql_local_node2"},
    3: {"cloud": "mysql_node3", "local": "mysql_local_node3"}
}


def _generate_cache_key(query, node):
    """
    Generate a unique cache key for a query and node combination.

    Args:
        query (str): SQL query string
        node (int): Node number (1, 2, or 3)

    Returns:
        str: MD5 hash of the query and node
    """
    # Normalize query: strip whitespace and convert to lowercase
    normalized_query = ' '.join(query.strip().lower().split())
    # Include node number in cache key
    cache_input = f"node{node}:{normalized_query}"
    return hashlib.md5(cache_input.encode()).hexdigest()


def _is_cache_valid(cache_entry):
    """
    Check if a cache entry is still valid based on TTL.

    Args:
        cache_entry (dict): Cache entry with 'timestamp' and 'data' keys

    Returns:
        bool: True if cache is valid, False if expired
    """
    if not CACHE_ENABLED:
        return False

    timestamp = cache_entry.get('timestamp')
    if not timestamp:
        return False

    age = (datetime.now() - timestamp).total_seconds()
    return age < CACHE_TTL_SECONDS


def get_node_config(node):
    """
    Get the configuration for a specific node.

    Args:
        node (int): Node number (1, 2, or 3)

    Returns:
        dict: Configuration dictionary for the specified node

    Raises:
        ValueError: If node number is invalid
    """
    if node not in NODE_CONFIGS:
        raise ValueError(f"Invalid node number: {node}. Must be 1, 2, or 3.")

    config_type = "cloud" if USE_CLOUD_SQL else "local"
    return NODE_CONFIGS[node][config_type]


def get_db_connection(node):
    """
    Establish and return a database connection for a specific node.

    Args:
        node (int): Node number (1, 2, or 3)

    Returns:
        mysql.connector.connection: Database connection object

    Raises:
        Exception: If connection fails
    """
    config = get_node_config(node)
    config_type = "Cloud SQL" if USE_CLOUD_SQL else "Local"

    try:
        conn = mysql.connector.connect(
            host=config["host"],
            port=config["port"],
            user=config["user"],
            password=config["password"],
            database=config["database"],
            connect_timeout=10  # Add timeout to prevent hanging
        )
        return conn
    except mysql.connector.Error as db_err:
        error_code = db_err.errno if hasattr(db_err, 'errno') else 'Unknown'
        error_msg = (
            f"Failed to connect to {config_type} database (Node {node})\n"
            f"Host: {config['host']}:{config['port']}\n"
            f"Database: {config['database']}\n"
            f"User: {config['user']}\n"
            f"Error Code: {error_code}\n"
            f"Error: {str(db_err)}\n\n"
            f"Common solutions:\n"
            f"1. Check if the database server is running\n"
            f"2. Verify host/port are correct\n"
            f"3. Ensure user has proper permissions\n"
            f"4. Check firewall/network settings\n"
            f"5. For Cloud SQL: verify IP whitelist and public IP access"
        )
        raise Exception(error_msg)
    except Exception as e:
        raise Exception(f"Failed to connect to {config_type} database (Node {node}) at {config['host']}:{config['port']}\n"
                      f"Error: {str(e)}")


def fetch_data(query, node, ttl=9999):
    """
    Execute a SQL query and return results as a pandas DataFrame from a specific node.
    Uses st.connection() when running in Streamlit for better caching and connection management.
    Falls back to direct MySQL connection when not in Streamlit.

    Args:
        query (str): SQL query to execute
        node (int): Node number (1, 2, or 3) to query from
        ttl (int): Time-to-live for cached results in seconds (default: 9999)

    Returns:
        pandas.DataFrame: Query results

    Raises:
        Exception: If query execution fails
    """
    # Validate node number
    if node not in NODE_CONFIGS:
        raise ValueError(f"Invalid node number: {node}. Must be 1, 2, or 3.")

    # Try to use Streamlit connection if available
    if _is_running_in_streamlit():
        conn_name = None
        try:
            import streamlit as st
            # Get the appropriate connection name for this node
            config_type = "cloud" if USE_CLOUD_SQL else "local"
            conn_name = STREAMLIT_CONN_NAMES[node][config_type]

            # Use st.connection for automatic caching and connection management
            conn = st.connection(conn_name, type='sql')
            # Execute query with built-in caching (ttl in seconds)
            return conn.query(query, ttl=ttl)
        except Exception as e:
            config = get_node_config(node)
            config_type_name = "Cloud SQL" if USE_CLOUD_SQL else "Local"
            conn_name_msg = f"[connections.{conn_name}]" if conn_name else "connection"
            error_msg = (
                f"Streamlit connection failed for {config_type_name} (Node {node}) "
                f"({config['host']}:{config['port']}/{config['database']}): {str(e)}\n"
                f"Error type: {type(e).__name__}\n\n"
                f"Make sure your secrets.toml has {conn_name_msg} configured correctly.\n"
                f"Current USE_CLOUD_SQL setting: {USE_CLOUD_SQL}"
            )
            raise Exception(error_msg)

    # Not in Streamlit - use manual connection with custom caching
    cache_key = _generate_cache_key(query, node)

    # Check if valid cached result
    if CACHE_ENABLED and cache_key in _query_cache:
        cache_entry = _query_cache[cache_key]
        if _is_cache_valid(cache_entry):
            return cache_entry['data'].copy()
        else:
            del _query_cache[cache_key]

    # Cache miss or expired - fetch from database
    conn = None
    cursor = None

    try:
        conn = get_db_connection(node)
        cursor = conn.cursor(dictionary=True)
        cursor.execute(query)
        data = cursor.fetchall()
        result_df = pd.DataFrame(data)

        # Store in cache
        if CACHE_ENABLED:
            _query_cache[cache_key] = {
                'timestamp': datetime.now(),
                'data': result_df.copy(),
                'query': query[:100],
                'node': node
            }

        return result_df

    except mysql.connector.Error as db_err:
        config = get_node_config(node)
        config_type = "Cloud SQL" if USE_CLOUD_SQL else "Local"
        error_msg = (
            f"Database error while fetching data from {config_type} (Node {node}) "
            f"({config['host']}:{config['port']}/{config['database']}): {str(db_err)}\n"
            f"Query: {query[:200]}..."
        )
        raise Exception(error_msg)

    except Exception as e:
        config = get_node_config(node)
        config_type = "Cloud SQL" if USE_CLOUD_SQL else "Local"
        error_msg = (
            f"Failed to fetch data from {config_type} (Node {node}) "
            f"({config['host']}:{config['port']}/{config['database']}): {str(e)}\n"
            f"Error type: {type(e).__name__}"
        )
        raise Exception(error_msg)

    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()

def execute_query(query, node, isolation_level="READ COMMITTED"):
    """
    Execute a write query (INSERT, UPDATE, DELETE) on specified database node
    
    Args:
        query: SQL query string (INSERT, UPDATE, DELETE)
        node: Node number (1, 2, or 3)
        isolation_level: Transaction isolation level
    
    Returns:
        Number of affected rows or True on success
    """
    # Validate node number
    if node not in NODE_CONFIGS:
        raise ValueError(f"Invalid node number: {node}. Must be 1, 2, or 3.")
    
    # Get config from centralized node config
    config = get_node_config(node)

    conn = None
    cursor = None
    
    try:
        # Connect to MySQL
        conn = mysql.connector.connect(**config)
        cursor = conn.cursor()
        
        # Set isolation level
        cursor.execute(f"SET SESSION TRANSACTION ISOLATION LEVEL {isolation_level}")
        
        # Start transaction
        cursor.execute("START TRANSACTION")
        
        # Execute the query
        cursor.execute(query)
        
        # Commit transaction
        conn.commit()
        
        # Return affected rows count
        affected_rows = cursor.rowcount
        
        return affected_rows
        
    except mysql.connector.Error as e:
        if conn:
            conn.rollback()
        raise Exception(f"MySQL Error for Node {node}: {str(e)}")
    
    except Exception as e:
        if conn:
            conn.rollback()
        raise Exception(f"Error executing query on Node {node}: {str(e)}")
    
    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()

def execute_multi_statement_query(query, node, ttl=3600):
    """
    Execute a multi-statement SQL query and return the final SELECT results from a specific node.
    Useful for queries that create temporary tables before selecting data.
    Results are cached to avoid repeated database queries.

    IMPORTANT: This function bypasses Streamlit's connection pooling to maintain
    a single connection across all statements (required for temporary tables).

    Args:
        query (str): Multi-statement SQL query (statements separated by semicolons)
        node (int): Node number (1, 2, or 3) to execute on
        ttl (int): Time-to-live for cached results in seconds (default: 3600)

    Returns:
        pandas.DataFrame: Results from the final SELECT statement (from cache or fresh)

    Raises:
        Exception: If query execution fails
    """
    # Validate node number
    if node not in NODE_CONFIGS:
        raise ValueError(f"Invalid node number: {node}. Must be 1, 2, or 3.")

    # Generate cache key
    cache_key = _generate_cache_key(query, node)

    # Check if we have a valid cached result
    if CACHE_ENABLED and cache_key in _query_cache:
        cache_entry = _query_cache[cache_key]
        if _is_cache_valid(cache_entry):
            # Return cached data (create a copy to prevent modifications)
            return cache_entry['data'].copy()
        else:
            # Remove expired cache entry
            del _query_cache[cache_key]

    # Cache miss or expired - fetch from database
    conn = None
    cursor = None
    config = get_node_config(node)
    config_type = "Cloud SQL" if USE_CLOUD_SQL else "Local"

    try:
        # Establish direct connection (not using st.connection to maintain single session)
        conn = mysql.connector.connect(
            host=config["host"],
            port=config["port"],
            user=config["user"],
            password=config["password"],
            database=config["database"],
            connect_timeout=30,  # Longer timeout for cloud connections
            autocommit=True,  # Important for temporary tables
            allow_local_infile=False,  # Security setting
            consume_results=True  # Automatically consume unread results
        )

        cursor = conn.cursor(dictionary=True, buffered=True)

        # Split the query into individual statements
        statements = [s.strip() for s in query.split(';') if s.strip()]

        if len(statements) == 0:
            raise Exception("No valid SQL statements found in query")

        # Execute all statements in sequence on the same connection
        # This ensures temporary tables persist across statements
        for i, statement in enumerate(statements[:-1]):
            try:
                cursor.execute(statement)
                # Try to consume any results
                try:
                    cursor.fetchall()
                except mysql.connector.errors.InterfaceError:
                    pass  # No results to fetch (e.g., CREATE, DROP statements)
            except mysql.connector.Error as stmt_err:
                raise Exception(f"Failed to execute statement {i+1}/{len(statements)} on Node {node}: {str(stmt_err)}\nStatement: {statement[:200]}")

        # Execute the final SELECT statement and fetch results
        try:
            cursor.execute(statements[-1])
            data = cursor.fetchall()
            result_df = pd.DataFrame(data)
        except mysql.connector.Error as stmt_err:
            raise Exception(f"Failed to execute final SELECT statement on Node {node}: {str(stmt_err)}\nStatement: {statements[-1][:200]}")

        # Store in cache
        if CACHE_ENABLED:
            _query_cache[cache_key] = {
                'timestamp': datetime.now(),
                'data': result_df.copy(),
                'query': query[:100],  # Store first 100 chars for debugging
                'node': node
            }

        return result_df

    except mysql.connector.Error as db_err:
        error_code = db_err.errno if hasattr(db_err, 'errno') else 'Unknown'
        error_msg = (
            f"Database error in multi-statement query ({config_type}, Node {node})\n"
            f"Host: {config['host']}:{config['port']}\n"
            f"Database: {config['database']}\n"
            f"Error Code: {error_code}\n"
            f"Error: {str(db_err)}\n\n"
            f"Query preview: {query[:300]}..."
        )
        raise Exception(error_msg)

    except Exception as e:
        error_msg = (
            f"Failed to execute multi-statement query on {config_type} (Node {node})\n"
            f"Host: {config['host']}:{config['port']}\n"
            f"Error type: {type(e).__name__}\n"
            f"Error: {str(e)}"
        )
        raise Exception(error_msg)

    finally:
        if cursor:
            try:
                cursor.close()
            except:
                pass
        if conn:
            try:
                conn.close()
            except:
                pass


def test_connection(node):
    """
    Test the database connection for a specific node.

    Args:
        node (int): Node number (1, 2, or 3) to test

    Returns:
        bool: True if connection successful, False otherwise
    """
    try:
        conn = get_db_connection(node)
        cursor = conn.cursor()
        cursor.execute("SELECT 1")
        cursor.fetchone()
        cursor.close()
        conn.close()
        print(f"Node {node} database connection successful!")
        return True
    except Exception as e:
        print(f"Node {node} database connection failed: {str(e)}")
        return False


# ============================================================================
# DISTRIBUTED LOCKING FUNCTIONS
# ============================================================================

# Initialize the distributed lock manager with node configurations
_lock_manager = None

def _get_lock_manager(current_node_id: str = "app"):
    """
    Get or create the distributed lock manager instance.

    Args:
        current_node_id: Identifier for this application instance

    Returns:
        DistributedLockManager instance
    """
    # Lazy import to avoid Streamlit module caching issues
    from python.utils.lock_manager import DistributedLockManager

    global _lock_manager
    if _lock_manager is None:
        # Build node configs for lock manager
        lock_node_configs = {}
        for node_num in [1, 2, 3]:
            config = get_node_config(node_num)
            lock_node_configs[node_num] = config
        _lock_manager = DistributedLockManager(lock_node_configs, current_node_id)
    return _lock_manager


def create_dedicated_connection(node: int, isolation_level: str = "REPEATABLE READ") -> mysql.connector.connection.MySQLConnection:
    """
    Create a dedicated connection with specific isolation level.
    Use this for concurrent transaction testing.

    Args:
        node: Node number (1, 2, or 3)
        isolation_level: Transaction isolation level

    Returns:
        MySQL connection with isolation level set
    """
    conn = get_db_connection(node)
    cursor = conn.cursor()
    cursor.execute(f"SET SESSION TRANSACTION ISOLATION LEVEL {isolation_level}")
    cursor.close()
    return conn


def execute_with_lock(query: str, params: tuple, resource_id: str,
                     target_node: int, isolation_level: str = "READ COMMITTED",
                     timeout: int = 30, current_node_id: str = "app") -> Dict[str, Any]:
    """
    Execute a write query with distributed locking.

    This method:
    1. Acquires lock on the resource
    2. Executes the query
    3. Commits the transaction
    4. Releases the lock

    Args:
        query: SQL query to execute (INSERT, UPDATE, DELETE)
        params: Query parameters (tuple)
        resource_id: Unique identifier for the resource being modified
        target_node: Node where the query should execute
        isolation_level: Transaction isolation level
        timeout: Lock acquisition timeout in seconds
        current_node_id: Identifier for this application instance

    Returns:
        dict with status, affected_rows, and message
    """
    lock_manager = _get_lock_manager(current_node_id)
    conn = None
    cursor = None

    try:
        # Acquire lock on the resource
        if not lock_manager.acquire_lock(resource_id, target_node, timeout):
            return {
                'status': 'failed',
                'error': f'Failed to acquire lock on {resource_id} at Node {target_node}',
                'affected_rows': 0
            }

        # Execute the query
        conn = create_dedicated_connection(target_node, isolation_level)
        cursor = conn.cursor()

        cursor.execute("START TRANSACTION")
        cursor.execute(query, params)
        affected_rows = cursor.rowcount
        conn.commit()

        return {
            'status': 'success',
            'affected_rows': affected_rows,
            'message': f'Query executed successfully on Node {target_node}'
        }

    except Exception as e:
        if conn:
            conn.rollback()
        return {
            'status': 'failed',
            'error': str(e),
            'affected_rows': 0
        }

    finally:
        # Always release the lock
        lock_manager.release_lock(resource_id, target_node)

        if cursor:
            cursor.close()
        if conn:
            conn.close()


def execute_multi_node_write(query: str, params: tuple, resource_id: str,
                             nodes: List[int], isolation_level: str = "READ COMMITTED",
                             timeout: int = 30, current_node_id: str = "app") -> Dict[str, Any]:
    """
    Execute the same write query across multiple nodes with distributed locking.

    This is for replication scenarios where the same update must happen on
    multiple nodes atomically.

    Args:
        query: SQL query to execute
        params: Query parameters
        resource_id: Unique identifier for the resource
        nodes: List of nodes where query should execute
        isolation_level: Transaction isolation level
        timeout: Lock acquisition timeout
        current_node_id: Identifier for this application instance

    Returns:
        dict with status and results per node
    """
    lock_manager = _get_lock_manager(current_node_id)
    results = {}

    try:
        # Acquire locks on all nodes
        if not lock_manager.acquire_multi_node_lock(resource_id, nodes, timeout):
            return {
                'status': 'failed',
                'error': 'Failed to acquire locks on all nodes',
                'results': {}
            }

        # Execute on all nodes
        for node in nodes:
            conn = None
            cursor = None

            try:
                conn = create_dedicated_connection(node, isolation_level)
                cursor = conn.cursor()

                cursor.execute("START TRANSACTION")
                cursor.execute(query, params)
                affected_rows = cursor.rowcount
                conn.commit()

                results[node] = {
                    'status': 'success',
                    'affected_rows': affected_rows
                }

            except Exception as e:
                if conn:
                    conn.rollback()

                results[node] = {
                    'status': 'failed',
                    'error': str(e)
                }

                # If any node fails, mark overall status as failed
                return {
                    'status': 'failed',
                    'error': f'Failed on Node {node}: {e}',
                    'results': results
                }

            finally:
                if cursor:
                    cursor.close()
                if conn:
                    conn.close()

        return {
            'status': 'success',
            'message': f'Query executed successfully on nodes {nodes}',
            'results': results
        }

    finally:
        # Always release locks on all nodes
        lock_manager.release_multi_node_lock(resource_id, nodes)


def replicate_write(query: str, params: tuple, resource_id: str,
                   source_node: int, isolation_level: str = "READ COMMITTED",
                   current_node_id: str = "app") -> Dict[str, Any]:
    """
    Execute a write on source node and replicate to appropriate target nodes.

    Replication strategy:
    - Node 1 (Central) -> replicate to both Node 2 and Node 3
    - Node 2 -> replicate to Node 1 (and Node 1 will replicate to Node 3)
    - Node 3 -> replicate to Node 1 (and Node 1 will replicate to Node 2)

    Args:
        query: SQL query to execute
        params: Query parameters
        resource_id: Unique identifier for the resource
        source_node: Node where write originates
        isolation_level: Transaction isolation level
        current_node_id: Identifier for this application instance

    Returns:
        dict with replication status
    """
    # Determine target nodes based on source
    if source_node == 1:
        # Central node -> replicate to both partitions
        target_nodes = [1, 2, 3]
    elif source_node == 2:
        # Node 2 -> update Node 2 and Central
        target_nodes = [2, 1]
    elif source_node == 3:
        # Node 3 -> update Node 3 and Central
        target_nodes = [3, 1]
    else:
        return {'status': 'failed', 'error': f'Invalid source node: {source_node}'}

    # Execute on all target nodes with distributed locking
    result = execute_multi_node_write(
        query=query,
        params=params,
        resource_id=resource_id,
        nodes=target_nodes,
        isolation_level=isolation_level,
        current_node_id=current_node_id
    )

    return result


def check_connectivity() -> Dict[int, bool]:
    """
    Check connectivity to all nodes.

    Returns:
        dict mapping node numbers to connectivity status
    """
    status = {}

    for node in [1, 2, 3]:
        try:
            conn = get_db_connection(node)
            cursor = conn.cursor()
            cursor.execute("SELECT 1")
            cursor.fetchone()
            cursor.close()
            conn.close()
            status[node] = True
        except Exception as e:
            print(f"Node {node} connectivity check failed: {e}")
            status[node] = False

    return status


def cleanup_locks(current_node_id: str = "app"):
    """
    Cleanup: release all locks held by this manager.
    Call this before shutting down the application.

    Args:
        current_node_id: Identifier for this application instance
    """
    lock_manager = _get_lock_manager(current_node_id)
    lock_manager.release_all_locks()


