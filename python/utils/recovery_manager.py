"""
Global Failure and Recovery System
Handles the 4 case studies for distributed database recovery
"""

import hashlib
import os
import time
from datetime import datetime
from typing import List, Dict, Optional
import mysql.connector
from mysql.connector import Error


class RecoveryManager:
    """Manages recovery logs and operations for distributed database system"""
    
    def __init__(self, db_config: Dict, current_node_id: int, max_retries: int = 3):
        """
        Initialize Recovery Manager
        
        Args:
            db_config: Database configuration dictionary
            current_node_id: Current node ID (1, 2, or 3)
            max_retries: Maximum retry attempts for recovery operations
        """
        self.db_config = db_config
        self.current_node_id = current_node_id
        self.max_retries = max_retries
    
    def generate_transaction_hash(self, target_node: int, source_node: int, sql_statement: str) -> str:
        """Generate unique hash to prevent duplicate recovery logs"""
        unique_string = f"{target_node}_{source_node}_{sql_statement}_{datetime.now().strftime('%Y%m%d')}"
        return hashlib.sha256(unique_string.encode()).hexdigest()
    
    def get_db_connection(self):
        """Get database connection for current node"""
        try:
            connection = mysql.connector.connect(**self.db_config)
            return connection
        except Error as e:
            print(f"Error connecting to database: {e}")
            raise
    
    def log_backup(self, target_node: int, source_node: int, sql_statement: str) -> bool:
        """
        Log failed replication transaction for recovery
        
        Args:
            target_node: ID of the failed node that needs this transaction
            source_node: ID of the node that generated the transaction  
            sql_statement: The SQL query that failed to replicate
            
        Returns:
            bool: True if logged successfully, False otherwise
        """
        connection = None
        cursor = None
        try:
            # Generate hash to prevent duplicates
            transaction_hash = self.generate_transaction_hash(target_node, source_node, sql_statement)
            
            connection = self.get_db_connection()
            cursor = connection.cursor()
            
            # Check if this transaction is already logged
            check_sql = """
                SELECT COUNT(*) FROM recovery_log 
                WHERE transaction_hash = %s AND status IN ('PENDING', 'COMPLETED')
            """
            cursor.execute(check_sql, (transaction_hash,))
            
            if cursor.fetchone()[0] > 0:
                print(f"Transaction already logged (hash: {transaction_hash[:8]}...)")
                return True
            
            # Insert recovery log
            insert_sql = """
                INSERT INTO recovery_log 
                (target_node, source_node, sql_statement, transaction_hash)
                VALUES (%s, %s, %s, %s)
            """
            
            cursor.execute(insert_sql, (target_node, source_node, sql_statement, transaction_hash))
            connection.commit()
            
            log_id = cursor.lastrowid
            print(f"Recovery log created: ID={log_id}, Target=Node{target_node}, Source=Node{source_node}")
            
            # Store in backup node as well (cross-backup)
            self._store_cross_backup(target_node, source_node, sql_statement, transaction_hash)
            
            return True
            
        except Error as e:
            print(f"Failed to log backup transaction: {e}")
            return False
        finally:
            if cursor:
                cursor.close()
            if connection:
                connection.close()
    
    def _store_cross_backup(self, target_node: int, source_node: int, sql_statement: str, transaction_hash: str):
        """Store backup log in another node to prevent single point of failure"""
        try:
            # Determine backup node (avoid source and target nodes, and current node)
            backup_node = None
            for node in [1, 2, 3]:
                if node != source_node and node != target_node and node != self.current_node_id:
                    backup_node = node
                    break
            
            if backup_node:
                print(f"Storing cross-backup in Node {backup_node} (original in Node {self.current_node_id})")
                
                # Get configuration for the backup node
                from python.db.db_config import get_node_config
                backup_config = get_node_config(backup_node)
                
                connection = None
                cursor = None
                try:
                    # Connect to the backup node
                    import mysql.connector
                    connection = mysql.connector.connect(**backup_config)
                    cursor = connection.cursor()
                    
                    insert_sql = """
                        INSERT INTO recovery_log 
                        (target_node, source_node, sql_statement, transaction_hash, error_message)
                        VALUES (%s, %s, %s, %s, %s)
                    """
                    
                    cursor.execute(insert_sql, (
                        target_node, 
                        source_node, 
                        sql_statement, 
                        transaction_hash,
                        f"CROSS_BACKUP_FROM_NODE_{self.current_node_id}"
                    ))
                    connection.commit()
                    print(f"Cross-backup successfully stored in Node {backup_node}")
                    
                except Exception as e:
                    print(f"Failed to store cross-backup in Node {backup_node}: {e}")
                finally:
                    if cursor:
                        cursor.close()
                    if connection:
                        connection.close()
            else:
                print(f"No available backup node found (source={source_node}, target={target_node}, current={self.current_node_id})")
                
        except Exception as e:
            print(f"Error in cross-backup process: {e}")
    
    def check_and_recover_pending_logs(self) -> Dict:
        """
        Check for pending recovery logs and attempt to recover them
        Called when node starts up or manually triggered
        Checks ALL available nodes for recovery logs targeting this node
        
        Returns:
            Dict: Recovery results summary
        """
        print(f"Node {self.current_node_id} checking for pending recovery logs across all nodes...")
        
        all_pending_logs = []
        recovery_results = {
            'total_logs': 0,
            'recovered': 0,
            'failed': 0,
            'skipped': 0,
            'nodes_checked': []
        }
        
        # Check all nodes (1, 2, 3) for recovery logs targeting this node
        for check_node_id in [1, 2, 3]:
            try:
                print(f"Checking Node {check_node_id} for recovery logs targeting Node {self.current_node_id}...")
                
                # Get node configuration
                from python.db.db_config import get_node_config
                node_config = get_node_config(check_node_id)
                
                connection = None
                cursor = None
                try:
                    connection = mysql.connector.connect(**node_config)
                    cursor = connection.cursor(dictionary=True)
                    
                    # Get pending recovery logs that target this node
                    select_sql = """
                        SELECT log_id, target_node, source_node, sql_statement, 
                               timestamp, retry_count, transaction_hash
                        FROM recovery_log 
                        WHERE status = 'PENDING' AND target_node = %s
                        ORDER BY timestamp ASC
                    """
                    
                    cursor.execute(select_sql, (self.current_node_id,))
                    pending_logs = cursor.fetchall()
                    
                    if pending_logs:
                        print(f"Found {len(pending_logs)} pending logs for Node {self.current_node_id} in Node {check_node_id}")
                        # Add the source node info to each log for reference
                        for log in pending_logs:
                            log['found_in_node'] = check_node_id
                        all_pending_logs.extend(pending_logs)
                    else:
                        print(f"No pending logs for Node {self.current_node_id} found in Node {check_node_id}")
                    
                    recovery_results['nodes_checked'].append(check_node_id)
                    
                except Error as e:
                    print(f"Could not connect to Node {check_node_id}: {e}")
                    # Node might be offline, continue checking other nodes
                    continue
                finally:
                    if cursor:
                        cursor.close()
                    if connection:
                        connection.close()
                        
            except Exception as e:
                print(f"Error checking Node {check_node_id}: {e}")
                continue
        
        # Sort all logs by timestamp to maintain chronological order
        all_pending_logs.sort(key=lambda x: x['timestamp'])
        
        # Deduplicate logs by transaction_hash to avoid processing the same transaction multiple times
        unique_logs = {}
        for log in all_pending_logs:
            tx_hash = log.get('transaction_hash', '')
            if tx_hash and tx_hash in unique_logs:
                print(f"Skipping duplicate log {log['log_id']} from Node {log['found_in_node']} (same hash as log {unique_logs[tx_hash]['log_id']} from Node {unique_logs[tx_hash]['found_in_node']})")
                # Mark the duplicate as completed to prevent re-processing
                self._mark_recovery_status_in_node(log['found_in_node'], log['log_id'], 'COMPLETED', "Duplicate transaction - skipped during deduplication")
                recovery_results['skipped'] += 1
            else:
                unique_logs[tx_hash] = log
        
        deduplicated_logs = list(unique_logs.values())
        recovery_results['total_logs'] = len(all_pending_logs)
        recovery_results['unique_logs'] = len(deduplicated_logs)
        
        if not deduplicated_logs:
            print(f"No unique pending recovery logs found for Node {self.current_node_id} after deduplication.")
            return recovery_results
        
        print(f"Found {len(all_pending_logs)} total logs, {len(deduplicated_logs)} unique after deduplication. Starting recovery...")
        
        # Process unique recovery logs
        for log in deduplicated_logs:
            print(f"Processing unique log from Node {log['found_in_node']}: {log['sql_statement'][:50]}...")
            result = self._attempt_recovery_cross_node(log)
            recovery_results[result] += 1
            
            # Small delay between recovery attempts
            time.sleep(0.1)
        
        print(f"Recovery completed: {recovery_results}")
        return recovery_results
    
    def _attempt_recovery(self, log: Dict) -> str:
        """
        Attempt to recover a single transaction log from current node
        
        Args:
            log: Recovery log record
            
        Returns:
            str: Recovery result ('recovered', 'failed', 'skipped')
        """
        log_id = log['log_id']
        target_node = log['target_node']
        sql_statement = log['sql_statement']
        retry_count = log['retry_count']
        transaction_hash = log.get('transaction_hash', '')
        
        connection = None
        cursor = None
        try:
            # Skip if max retries exceeded
            if retry_count >= self.max_retries:
                self._mark_recovery_status(log_id, 'FAILED', f"Max retries ({self.max_retries}) exceeded")
                return 'failed'
            
            # Skip if this is not the target node
            if target_node != self.current_node_id:
                return 'skipped'
            
            print(f"Attempting recovery for log {log_id}: {sql_statement[:50]}...")
            
            # Execute the recovery transaction
            connection = self.get_db_connection()
            cursor = connection.cursor()
            
            # Execute the failed SQL statement
            cursor.execute(sql_statement)
            connection.commit()
            
            # Mark as completed
            self._mark_recovery_status(log_id, 'COMPLETED', "Recovery successful")
            
            print(f"Successfully recovered log {log_id}")
            return 'recovered'
            
        except Error as e:
            # Check if this is a duplicate entry error (indicates transaction was already processed)
            if e.errno == 1062:  # MySQL duplicate entry error
                print(f"Transaction already exists in database - marking as completed (hash: {transaction_hash[:8]}...)")
                # Mark as completed since the data is already there
                self._mark_recovery_status(log_id, 'COMPLETED', "Transaction already exists - duplicate detected")
                return 'skipped'
            
            error_msg = f"Recovery attempt {retry_count + 1} failed: {str(e)}"
            
            # Increment retry count
            self._increment_retry_count(log_id, error_msg)
            
            print(f"Recovery failed for log {log_id}: {error_msg}")
            return 'failed'
        finally:
            if cursor:
                cursor.close()
            if connection:
                connection.close()
    
    def _attempt_recovery_cross_node(self, log: Dict) -> str:
        """
        Attempt to recover a single transaction log found in another node
        
        Args:
            log: Recovery log record with 'found_in_node' field
            
        Returns:
            str: Recovery result ('recovered', 'failed', 'skipped')
        """
        log_id = log['log_id']
        target_node = log['target_node']
        source_node = log.get('found_in_node', self.current_node_id)
        sql_statement = log['sql_statement']
        retry_count = log['retry_count']
        transaction_hash = log.get('transaction_hash', '')
        
        connection = None
        cursor = None
        try:
            # Skip if max retries exceeded
            if retry_count >= self.max_retries:
                self._mark_recovery_status_in_node(source_node, log_id, 'FAILED', f"Max retries ({self.max_retries}) exceeded")
                return 'failed'
            
            # Skip if this is not the target node
            if target_node != self.current_node_id:
                return 'skipped'
            
            # Check if this transaction was already completed by checking transaction hash
            connection = self.get_db_connection()
            cursor = connection.cursor()
            
            # Check if we already have this transaction completed locally
            if transaction_hash:
                check_sql = """
                    SELECT COUNT(*) FROM recovery_log 
                    WHERE transaction_hash = %s AND status = 'COMPLETED'
                """
                cursor.execute(check_sql, (transaction_hash,))
                
                if cursor.fetchone()[0] > 0:
                    print(f"Transaction hash {transaction_hash[:8]}... already completed - marking as skipped")
                    # Mark the duplicate log as completed to avoid re-processing
                    self._mark_recovery_status_in_node(source_node, log_id, 'COMPLETED', "Duplicate transaction - already processed")
                    return 'skipped'
            
            print(f"Attempting cross-node recovery for log {log_id} from Node {source_node}: {sql_statement[:50]}...")
            
            # Execute the failed SQL statement
            cursor.execute(sql_statement)
            connection.commit()
            
            # Mark as completed in the source node where the log was found
            self._mark_recovery_status_in_node(source_node, log_id, 'COMPLETED', "Cross-node recovery successful")
            
            print(f"Successfully recovered cross-node log {log_id} from Node {source_node}")
            return 'recovered'
            
        except Error as e:
            # Check if this is a duplicate entry error (indicates transaction was already processed)
            if e.errno == 1062:  # MySQL duplicate entry error
                print(f"Transaction already exists in database - marking as completed (hash: {transaction_hash[:8]}...)")
                # Mark as completed since the data is already there
                self._mark_recovery_status_in_node(source_node, log_id, 'COMPLETED', "Transaction already exists - duplicate detected")
                return 'skipped'
            
            error_msg = f"Cross-node recovery attempt {retry_count + 1} failed: {str(e)}"
            
            # Increment retry count in the source node
            self._increment_retry_count_in_node(source_node, log_id, error_msg)
            
            print(f"Cross-node recovery failed for log {log_id} from Node {source_node}: {error_msg}")
            return 'failed'
        finally:
            if cursor:
                cursor.close()
            if connection:
                connection.close()
    
    def _mark_recovery_status(self, log_id: int, status: str, error_message: str = None):
        """Mark recovery log with final status in current node"""
        connection = None
        cursor = None
        try:
            connection = self.get_db_connection()
            cursor = connection.cursor()
            
            update_sql = """
                UPDATE recovery_log 
                SET status = %s, error_message = %s
                WHERE log_id = %s
            """
            
            cursor.execute(update_sql, (status, error_message, log_id))
            connection.commit()
            
        except Error as e:
            print(f"Failed to update recovery status: {e}")
        finally:
            if cursor:
                cursor.close()
            if connection:
                connection.close()
    
    def _mark_recovery_status_in_node(self, node_id: int, log_id: int, status: str, error_message: str = None):
        """Mark recovery log with final status in specified node"""
        connection = None
        cursor = None
        try:
            # Get configuration for the specified node
            from python.db.db_config import get_node_config
            node_config = get_node_config(node_id)
            
            connection = mysql.connector.connect(**node_config)
            cursor = connection.cursor()
            
            update_sql = """
                UPDATE recovery_log 
                SET status = %s, error_message = %s
                WHERE log_id = %s
            """
            
            cursor.execute(update_sql, (status, error_message, log_id))
            connection.commit()
            
        except Error as e:
            print(f"Failed to update recovery status in Node {node_id}: {e}")
        finally:
            if cursor:
                cursor.close()
            if connection:
                connection.close()
    
    def _increment_retry_count(self, log_id: int, error_message: str):
        """Increment retry count for recovery log in current node"""
        connection = None
        cursor = None
        try:
            connection = self.get_db_connection()
            cursor = connection.cursor()
            
            update_sql = """
                UPDATE recovery_log 
                SET retry_count = retry_count + 1, error_message = %s
                WHERE log_id = %s
            """
            
            cursor.execute(update_sql, (error_message, log_id))
            connection.commit()
            
        except Error as e:
            print(f"Failed to increment retry count: {e}")
        finally:
            if cursor:
                cursor.close()
            if connection:
                connection.close()
    
    def _increment_retry_count_in_node(self, node_id: int, log_id: int, error_message: str):
        """Increment retry count for recovery log in specified node"""
        connection = None
        cursor = None
        try:
            # Get configuration for the specified node
            from python.db.db_config import get_node_config
            node_config = get_node_config(node_id)
            
            connection = mysql.connector.connect(**node_config)
            cursor = connection.cursor()
            
            update_sql = """
                UPDATE recovery_log 
                SET retry_count = retry_count + 1, error_message = %s
                WHERE log_id = %s
            """
            
            cursor.execute(update_sql, (error_message, log_id))
            connection.commit()
            
        except Error as e:
            print(f"Failed to increment retry count in Node {node_id}: {e}")
        finally:
            if cursor:
                cursor.close()
            if connection:
                connection.close()
    
    def get_recovery_status(self) -> Dict:
        """Get recovery logs status summary from current node"""
        connection = None
        cursor = None
        try:
            connection = self.get_db_connection()
            cursor = connection.cursor()
            
            status_sql = """
                SELECT status, COUNT(*) as count
                FROM recovery_log 
                GROUP BY status
            """
            
            cursor.execute(status_sql)
            results = cursor.fetchall()
            
            status_summary = {'PENDING': 0, 'COMPLETED': 0, 'FAILED': 0}
            for status, count in results:
                status_summary[status] = count
                
            return status_summary
            
        except Error as e:
            print(f"Error getting recovery status: {e}")
            return {}
        finally:
            if cursor:
                cursor.close()
            if connection:
                connection.close()
    
    def get_global_recovery_status(self) -> Dict:
        """Get global recovery status across all nodes"""
        connection = None
        cursor = None
        try:
            connection = self.get_db_connection()
            cursor = connection.cursor(dictionary=True)
            
            # Get checkpoint information
            cursor.execute("""
                SELECT node_id, last_processed_log_id
                FROM recovery_checkpoints
                WHERE node_id IN (1, 2, 3)
                ORDER BY node_id
            """)
            
            checkpoints = cursor.fetchall()
            return {
                'status': 'success',
                'checkpoints': checkpoints,
                'timestamp': datetime.now().isoformat()
            }
            
        except Error as e:
            return {
                'status': 'error',
                'message': str(e),
                'timestamp': datetime.now().isoformat()
            }
        finally:
            if cursor:
                cursor.close()
            if connection:
                connection.close()

    def _quick_count_pending_logs(self) -> int:
        """Quick count of pending recovery logs across all nodes without acquiring locks"""
        total = 0
        try:
            from python.db.db_config import get_db_connection

            # Check Node 1 only (central node) for a quick estimate
            # This is faster than checking all nodes
            conn = get_db_connection(1)
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM recovery_log WHERE status = 'PENDING'")
            result = cursor.fetchone()
            total = result[0] if result else 0
            cursor.close()
            conn.close()

        except Exception as e:
            print(f"Quick count check failed: {e}")
            # Return -1 to signal check failure (proceed with full recovery)
            return -1

        return total

    def create_checkpoint_table_if_not_exists(self):
        """Create global checkpoint table if it doesn't exist"""
        connection = None
        cursor = None
        try:
            connection = self.get_db_connection()
            cursor = connection.cursor()
            
            create_table_sql = """
                CREATE TABLE IF NOT EXISTS recovery_checkpoints (
                    node_id INT PRIMARY KEY,
                    last_processed_log_id INT DEFAULT 0
                )
            """
            cursor.execute(create_table_sql)
            
            # Initialize checkpoints for all nodes
            for node in [1, 2, 3]:
                cursor.execute("""
                    INSERT IGNORE INTO recovery_checkpoints (node_id, last_processed_log_id) 
                    VALUES (%s, 0)
                """, (node,))
            
            connection.commit()
            print("Recovery checkpoint table initialized")
            
        except Error as e:
            print(f"Error creating checkpoint table: {e}")
        finally:
            if cursor:
                cursor.close()
            if connection:
                connection.close()
    
    def acquire_global_recovery_lock(self, timeout_seconds=30) -> bool:
        """Acquire global recovery lock to prevent concurrent recovery operations"""
        connection = None
        cursor = None
        try:
            connection = self.get_db_connection()
            cursor = connection.cursor()
            
            # Try to acquire lock by inserting a special lock record
            # Create lock record if not exists (node_id = 0 is reserved for locking)
            cursor.execute("""
                INSERT IGNORE INTO recovery_checkpoints (node_id, last_processed_log_id) 
                VALUES (0, -1)
            """)
            
            # Try to update the lock record to claim it
            cursor.execute("""
                UPDATE recovery_checkpoints 
                SET last_processed_log_id = %s 
                WHERE node_id = 0 AND last_processed_log_id = -1
            """, (os.getpid(),))  # Use process ID as lock identifier
            
            connection.commit()
            return cursor.rowcount > 0
            
        except Error as e:
            print(f"Error acquiring recovery lock: {e}")
            return False
        finally:
            if cursor:
                cursor.close()
            if connection:
                connection.close()
    
    def release_global_recovery_lock(self):
        """Release global recovery lock"""
        connection = None
        cursor = None
        try:
            connection = self.get_db_connection()
            cursor = connection.cursor()
            
            # Reset the lock record back to -1 (available)
            cursor.execute("""
                UPDATE recovery_checkpoints 
                SET last_processed_log_id = -1 
                WHERE node_id = 0 AND last_processed_log_id = %s
            """, (os.getpid(),))
            
            connection.commit()
            
        except Error as e:
            print(f"Error releasing recovery lock: {e}")
        finally:
            if cursor:
                cursor.close()
            if connection:
                connection.close()
    
    def get_global_checkpoints(self) -> Dict[int, int]:
        """Get current global checkpoints for all nodes"""
        connection = None
        cursor = None
        try:
            connection = self.get_db_connection()
            cursor = connection.cursor(dictionary=True)
            
            cursor.execute("""
                SELECT node_id, last_processed_log_id 
                FROM recovery_checkpoints 
                WHERE node_id IN (1, 2, 3)
            """)
            
            results = cursor.fetchall()
            checkpoints = {}
            for row in results:
                checkpoints[row['node_id']] = row['last_processed_log_id']
            
            # Ensure all nodes have checkpoints
            for node in [1, 2, 3]:
                if node not in checkpoints:
                    checkpoints[node] = 0
                    
            return checkpoints
            
        except Error as e:
            print(f"Error getting checkpoints: {e}")
            return {1: 0, 2: 0, 3: 0}
        finally:
            if cursor:
                cursor.close()
            if connection:
                connection.close()
    
    def update_checkpoint(self, node_id: int, last_processed_log_id: int):
        """Update checkpoint for a specific node"""
        connection = None
        cursor = None
        try:
            connection = self.get_db_connection()
            cursor = connection.cursor()
            
            cursor.execute("""
                UPDATE recovery_checkpoints 
                SET last_processed_log_id = %s
                WHERE node_id = %s
            """, (last_processed_log_id, node_id))
            
            connection.commit()
            
        except Error as e:
            print(f"Error updating checkpoint for node {node_id}: {e}")
        finally:
            if cursor:
                cursor.close()
            if connection:
                connection.close()
    
    def get_new_recovery_logs_since_checkpoint(self, node_id: int, checkpoint: int) -> List[Dict]:
        """Get recovery logs from a specific node since the last checkpoint"""
        connection = None
        cursor = None
        try:
            # Connect to the specific node to get its recovery logs
            from python.db.db_config import get_node_config, get_db_connection
            
            node_config = get_node_config(node_id)
            connection = get_db_connection(node_id)
            cursor = connection.cursor(dictionary=True)
            
            # Get logs with log_id greater than checkpoint and status = 'PENDING'
            cursor.execute("""
                SELECT log_id, target_node, source_node, sql_statement, transaction_hash, 
                       timestamp, status, retry_count, error_message
                FROM recovery_log 
                WHERE log_id > %s AND status = 'PENDING'
                ORDER BY log_id ASC
            """, (checkpoint,))
            
            logs = cursor.fetchall()
            
            # Add source node info to each log
            for log in logs:
                log['found_in_node'] = node_id
            
            return logs
            
        except Error as e:
            print(f"Error getting new recovery logs from node {node_id} since checkpoint {checkpoint}: {e}")
            return []
        finally:
            if cursor:
                cursor.close()
            if connection:
                connection.close()
    
    def process_recovery_logs_with_global_checkpoints(self) -> Dict:
        """Process recovery logs using global checkpoints with concurrency control"""
        recovery_results = {
            'total_logs': 0,
            'recovered': 0,
            'failed': 0,
            'lock_acquired': False,
            'nodes_processed': [],
            'checkpoint_updates': {}
        }
        
        # Quick pre-check: count pending logs across all nodes before acquiring lock
        # This avoids expensive lock acquisition when there's nothing to recover
        try:
            total_pending = self._quick_count_pending_logs()
            if total_pending == 0:
                print("Quick check: No pending recovery logs found. Skipping recovery.")
                return recovery_results
        except Exception as e:
            print(f"Quick check failed, proceeding with full recovery: {e}")

        # Ensure checkpoint table exists
        self.create_checkpoint_table_if_not_exists()
        
        # Try to acquire global lock
        if not self.acquire_global_recovery_lock():
            print("Another process is already running recovery. Skipping...")
            return recovery_results
        
        recovery_results['lock_acquired'] = True
        
        try:
            # Get current global checkpoints
            checkpoints = self.get_global_checkpoints()
            print(f"Current recovery checkpoints: {checkpoints}")
            
            # Process each node
            for node_id in [1, 2, 3]:
                try:
                    current_checkpoint = checkpoints[node_id]
                    new_logs = self.get_new_recovery_logs_since_checkpoint(node_id, current_checkpoint)
                    
                    if not new_logs:
                        print(f"No new recovery logs for Node {node_id} since checkpoint {current_checkpoint}")
                        continue
                    
                    print(f"Found {len(new_logs)} new recovery logs for Node {node_id}")
                    recovery_results['nodes_processed'].append(node_id)
                    recovery_results['total_logs'] += len(new_logs)
                    
                    # Process logs sequentially and track consecutive successes
                    last_consecutive_success = current_checkpoint
                    
                    for log in new_logs:
                        try:
                            result = self._attempt_recovery_cross_node(log)
                            
                            # Check for successful recovery (success, recovered, or skipped are all considered successful)
                            if result in ['success', 'recovered', 'skipped']:
                                recovery_results['recovered'] += 1
                                print(f"Successfully processed log {log['log_id']} from Node {node_id} (status: {result})")
                                
                                # Only update consecutive checkpoint if this log immediately follows the last processed
                                if log['log_id'] == last_consecutive_success + 1:
                                    last_consecutive_success = log['log_id']
                                # If there's a gap, don't update consecutive checkpoint but continue processing
                                    
                            else:
                                recovery_results['failed'] += 1
                                print(f"Failed to recover log {log['log_id']} from Node {node_id}: {result}")
                                # Failed log breaks the consecutive chain - continue processing but don't update checkpoint
                                
                        except Exception as log_error:
                            recovery_results['failed'] += 1
                            print(f"Exception processing log {log['log_id']}: {log_error}")
                            # Exception breaks the consecutive chain
                    
                    # Update checkpoint only to the highest consecutive successful log_id
                    if last_consecutive_success > current_checkpoint:
                        self.update_checkpoint(node_id, last_consecutive_success)
                        recovery_results['checkpoint_updates'][node_id] = last_consecutive_success
                        print(f"Updated Node {node_id} checkpoint to {last_consecutive_success} (consecutive successes)")
                        
                        # Show information about any gaps
                        if recovery_results['failed'] > 0:
                            remaining_logs = [log['log_id'] for log in new_logs if log['log_id'] > last_consecutive_success]
                            if remaining_logs:
                                print(f"Note: Logs {remaining_logs} will be retried in next recovery cycle due to earlier failures")
                    else:
                        print(f"Node {node_id} checkpoint unchanged at {current_checkpoint} - no consecutive successes")
                
                except Exception as node_error:
                    print(f"Error processing Node {node_id}: {node_error}")
                    continue
            
            print(f"Global recovery completed: {recovery_results}")
            return recovery_results
            
        finally:
            # Always release the lock
            self.release_global_recovery_lock()
        """Get recovery logs status summary across all nodes"""
        global_status = {
            'nodes': {},
            'total': {'PENDING': 0, 'COMPLETED': 0, 'FAILED': 0}
        }
        
        # Check all nodes
        for node_id in [1, 2, 3]:
            try:
                from python.db.db_config import get_node_config
                node_config = get_node_config(node_id)
                
                connection = None
                cursor = None
                try:
                    connection = mysql.connector.connect(**node_config)
                    cursor = connection.cursor()
                    
                    status_sql = """
                        SELECT status, COUNT(*) as count
                        FROM recovery_log 
                        GROUP BY status
                    """
                    
                    cursor.execute(status_sql)
                    results = cursor.fetchall()
                    
                    node_status = {'PENDING': 0, 'COMPLETED': 0, 'FAILED': 0}
                    for status, count in results:
                        node_status[status] = count
                        global_status['total'][status] += count
                    
                    global_status['nodes'][f'node_{node_id}'] = node_status
                    
                except Error as e:
                    global_status['nodes'][f'node_{node_id}'] = {'error': f'Could not connect: {str(e)}'}
                finally:
                    if cursor:
                        cursor.close()
                    if connection:
                        connection.close()
                        
            except Exception as e:
                global_status['nodes'][f'node_{node_id}'] = {'error': f'Configuration error: {str(e)}'}
        
        return global_status


# Global Recovery Function

def execute_global_recovery() -> Dict:
    """
    Execute global recovery across all nodes using checkpoints
    This is the main function to call before transactions
    """
    try:
        # Use Node 1 config as the recovery manager base
        from python.db.db_config import get_node_config
        node1_config = get_node_config(1)
        recovery_manager = RecoveryManager(node1_config, 1)
        
        return recovery_manager.process_recovery_logs_with_global_checkpoints()
        
    except Exception as e:
        print(f"Global recovery execution failed: {e}")
        return {
            'total_logs': 0,
            'recovered': 0,
            'failed': 0,
            'lock_acquired': False,
            'error': str(e)
        }

# Utility Functions for Integration

def execute_sql_on_local_db(sql_statement: str, db_config: Dict) -> bool:
    """
    Execute SQL statement on local database
    Used by log_backup function
    """
    connection = None
    cursor = None
    try:
        connection = mysql.connector.connect(**db_config)
        cursor = connection.cursor()
        cursor.execute(sql_statement)
        connection.commit()
        return True
    except Error as e:
        print(f"SQL execution failed: {e}")
        return False
    finally:
        if cursor:
            cursor.close()
        if connection:
            connection.close()


def simulate_replication_failure(target_node_id: int, source_node_id: int, sql_query: str, 
                               recovery_manager: RecoveryManager) -> Dict:
    """
    Simulate replication failure and log for recovery
    
    Returns:
        Dict: Status of the replication attempt
    """
    try:
        # Simulate attempting to replicate to target node
        # This is where you would normally try to execute on target node
        # For simulation, we'll assume it fails
        
        print(f"Simulating replication failure: Node {source_node_id} → Node {target_node_id}")
        print(f"Failed SQL: {sql_query}")
        
        # Log the failure for recovery
        success = recovery_manager.log_backup(target_node_id, source_node_id, sql_query)
        
        if success:
            return {
                'status': 'error',
                'message': f'Replication to Node {target_node_id} failed. Transaction logged for recovery.',
                'logged': True
            }
        else:
            return {
                'status': 'error', 
                'message': f'Replication to Node {target_node_id} failed. Failed to log for recovery!',
                'logged': False
            }
            
    except Exception as e:
        return {
            'status': 'error',
            'message': f'Replication error: {str(e)}',
            'logged': False
        }


def replicate_transaction(query: str, source_node: int, target_node: int, isolation_level: str = "READ COMMITTED") -> Dict:
    """
    Replicate a transaction from source node to target node with recovery logging
    
    Args:
        query: SQL statement to replicate
        source_node: Node that originated the transaction
        target_node: Node to replicate to
        isolation_level: Transaction isolation level
        
    Returns:
        Dict: Replication result with status and message
    """
    from python.db.db_config import get_node_config, create_dedicated_connection
    
    try:
        print(f"Attempting replication: Node {source_node} -> Node {target_node}")
        
        # Create connection to target node
        conn = create_dedicated_connection(target_node, isolation_level)
        cursor = conn.cursor()
        
        # Execute the replication query
        cursor.execute(query)
        conn.commit()
        cursor.close()
        conn.close()
        
        print(f"Replication successful: Node {source_node} -> Node {target_node}")
        return {
            'status': 'success',
            'message': f'Successfully replicated to Node {target_node}',
            'logged': False
        }
        
    except Exception as e:
        print(f"Replication failed: Node {source_node} -> Node {target_node}: {str(e)}")
        
        # Log the failure for recovery
        try:
            # Get source node config for recovery manager
            source_config = get_node_config(source_node)
            recovery_manager = RecoveryManager(source_config, source_node)
            
            success = recovery_manager.log_backup(target_node, source_node, query)
            
            return {
                'status': 'error',
                'message': f'Replication to Node {target_node} failed: {str(e)}',
                'error_details': str(e),
                'logged': success,
                'recovery_action': f'Transaction logged for recovery on Node {target_node}' if success else 'Failed to log for recovery'
            }
            
        except Exception as log_error:
            return {
                'status': 'error', 
                'message': f'Replication to Node {target_node} failed: {str(e)}',
                'error_details': str(e),
                'logged': False,
                'recovery_action': f'Recovery logging also failed: {str(log_error)}'
            }


# Example usage for the 4 case studies
if __name__ == "__main__":
    # Example database config
    db_config = {
        'host': 'localhost',
        'port': 3306,
        'user': 'user', 
        'password': 'rootpass',
        'database': 'node1_db'
    }
    
    # Initialize recovery manager for Node 1
    recovery_manager = RecoveryManager(db_config, current_node_id=1)
    
    # Case 1 & 3: Simulate replication failures
    print("=== Case 1: Node 2 → Node 1 replication failure ===")
    simulate_replication_failure(
        target_node_id=1, 
        source_node_id=2, 
        sql_query="INSERT INTO movies (title, year) VALUES ('Test Movie', 2025)",
        recovery_manager=recovery_manager
    )
    
    # Case 2 & 4: Check and recover when node comes online
    print("\n=== Case 2: Node 1 recovery check ===")
    recovery_results = recovery_manager.check_and_recover_pending_logs()
    print(f"Recovery results: {recovery_results}")
    
    # Check status
    print("\n=== Recovery Status Summary ===")
    status = recovery_manager.get_recovery_status()
    print(f"Status summary: {status}")